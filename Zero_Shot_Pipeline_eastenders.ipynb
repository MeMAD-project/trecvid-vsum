{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yTBP_QYuu6tc"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install transformers==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TiU_ES5tzpMH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:35:29.180007 139830866601792 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
      "I0802 19:35:29.203221 139830866601792 driver.py:124] Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "spkccRiv0CB3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:13.281779 139830866601792 filelock.py:274] Lock 139825313352056 acquired on /root/.cache/torch/transformers/c2341a51039a311cb3c7dc71b3d21970e6a127876f067f379f8bcd77ef870389.6a09face0659d64f93c9919f323e2ad4543ca9af5d2417b1bfb1a36f2f6b94a4.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adc925233be423e84534217af8de04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=473.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:13.714730 139830866601792 filelock.py:318] Lock 139825313352056 released on /root/.cache/torch/transformers/c2341a51039a311cb3c7dc71b3d21970e6a127876f067f379f8bcd77ef870389.6a09face0659d64f93c9919f323e2ad4543ca9af5d2417b1bfb1a36f2f6b94a4.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:14.134128 139830866601792 filelock.py:274] Lock 139825313352056 acquired on /root/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8839520cc89741789533e865e04bd156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:14.868336 139830866601792 filelock.py:318] Lock 139825313352056 released on /root/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:15.289589 139830866601792 filelock.py:274] Lock 139825313055688 acquired on /root/.cache/torch/transformers/414816afc2ab8922d082f893dbf90bcb9a43f09838039249c6c8ca3e8b77921f.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a03f06e14b499b981f2db92c11e807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:15.765900 139830866601792 filelock.py:318] Lock 139825313055688 released on /root/.cache/torch/transformers/414816afc2ab8922d082f893dbf90bcb9a43f09838039249c6c8ca3e8b77921f.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:16.298171 139830866601792 filelock.py:274] Lock 139825312594072 acquired on /root/.cache/torch/transformers/3efcb155a9475fe6b9318b8a8d5278bce1972d30291f97f2a8faeb50d02acabc.087b9fac49619019e540876a2d8ecb497884246b5aa8c9e8b7a0292cfbbe7c52.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bca545348245538468259ea21ea66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=260793700.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 19:36:40.389094 139830866601792 filelock.py:318] Lock 139825312594072 released on /root/.cache/torch/transformers/3efcb155a9475fe6b9318b8a8d5278bce1972d30291f97f2a8faeb50d02acabc.087b9fac49619019e540876a2d8ecb497884246b5aa8c9e8b7a0292cfbbe7c52.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0) # to utilize GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWiovVJG9ei_"
   },
   "source": [
    "We can use this pipeline by passing in a sequence and a list of candidate labels. The pipeline assumes by default that only one of the candidate labels is true, returning a list of scores for each label which add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_screenplays_scenes='./transcripts/shot-aligned_transcripts_2021_Max_Jack_Tanya.csv'\n",
    "path_screenplays_scenes='./transcripts/shot-aligned_transcripts_2021_Peggy_Archie.csv'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "hkfE6NRA0Dzy",
    "outputId": "8b3f9e37-3e46-4b25-813b-c5fa7bbc3c97"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_results(df, classifier, topic_labels):\n",
    "    df['topic'] = None\n",
    "    df['score'] = None\n",
    "    \n",
    "    for i in tqdm(df.index):\n",
    "        text = df.loc[i, 'transcript']\n",
    "        if text is not None:\n",
    "            result = classifier(text, topic_labels)\n",
    "            #df.loc[i, 'topic'] = str(str(result['labels'][0])+\"+\"+str(result['labels'][1]))\n",
    "            #df.loc[i, 'score'] = str(str(result['scores'][0])+\"+\"+str(result['scores'][1]) \n",
    "            df.loc[i, 'topic'] = result['labels'][0]\n",
    "            df.loc[i, 'score'] = result['scores'][0]\n",
    "            #if result['labels'][0]=='surprise':\n",
    "                #df.loc[i, 'score_hf'] = result['scores'][0]\n",
    "            #else:\n",
    "                #df.loc[i, 'score_hf'] = -1\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 6554.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death relative\n",
      "100\n",
      "Divorce\n",
      "73\n",
      "Marital separation\n",
      "65\n",
      "Imprisonment\n",
      "63\n",
      "injury illness\n",
      "53\n",
      "Marriage\n",
      "50\n",
      "Fired\n",
      "47\n",
      "Marital reconciliation\n",
      "45\n",
      "Retirement\n",
      "45\n",
      "Pregnancy\n",
      "40\n",
      "Sexual difficulties\n",
      "39\n",
      "New family member\n",
      "39\n",
      "Business readjustment\n",
      "39\n",
      "Money change\n",
      "38\n",
      "Work change\n",
      "36\n",
      "Arguing\n",
      "35\n",
      "Mortgage\n",
      "32\n",
      "Child leaving home\n",
      "29\n",
      "Trouble with in-laws\n",
      "29\n",
      "Achievement\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_labels_df=pd.read_csv('life_events_scale.csv',sep='\\t', names=['topic','weight'])\n",
    "candidate_labels_df=candidate_labels_df\n",
    "\n",
    "\n",
    "candidate_labels=dict(zip(candidate_labels_df.topic, candidate_labels_df.weight))\n",
    "for topic,weight in tqdm(candidate_labels.items()):\n",
    "    print(topic)\n",
    "    print(weight)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soap_opera_scale={'extramarital affair': 1/1.98, 'get divorced': 1/1.96,'illegitimate child': 1/1.45,'institutionalized for emotional problem': 1/1.43,'happily married': 1/4.05,'serious accident': 1/2.96,'murdered': 1/1.81,'attempt suicide': 1/1.26,'blackmailed': 1/1.86,'unfaithful spouse': 1/2.23,'sexually assaulted': 1/2.60,'abortion': 1/1.41}\n",
    "#candidate_labels={'death relative':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_several_binary(df, classifier, topic_labels):\n",
    "    for topic,weight in tqdm(topic_labels.items()):\n",
    "        df[topic] = None\n",
    "        for i in df.index:\n",
    "            text = df.loc[i, 'transcript']\n",
    "            if text is not None:\n",
    "                result = classifier(text,topic,multi_class=False)\n",
    "                df.loc[i, topic] = result['scores'][0]*weight\n",
    "        #df.to_csv('death_relative_Max_Jack_Tanya.csv')\n",
    "        df.to_csv('life_scale_ranking_end_Peggy_Archie.csv')\n",
    "        \n",
    "\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0        begin          end                 filename  shot_id  \\\n",
      "5            5  00:00:26:14  00:00:27:16  5300410550331962313.xml        6   \n",
      "8            8  00:00:31:15  00:00:35:05  5300410550331962313.xml        9   \n",
      "9            9  00:00:35:06  00:00:36:12  5300410550331962313.xml       10   \n",
      "12          12  00:00:38:16  00:00:42:24  5300410550331962313.xml       13   \n",
      "13          13  00:00:43:00  00:00:45:14  5300410550331962313.xml       14   \n",
      "\n",
      "                                           transcript  \n",
      "5                  I think you'll find that's for me!  \n",
      "8   You're not gonna get a tip,  if that's what yo...  \n",
      "9                      Just sign here, love, please.   \n",
      "12  Well, I know that outfit  ain't for my benefit...  \n",
      "13  Why do men think everything  revolves around t...  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(path_screenplays_scenes)\n",
    "df_with_text=df.dropna()\n",
    "print(df_with_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "100%|██████████| 20/20 [7:33:03<00:00, 1359.18s/it]  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        begin          end                 filename  shot_id  \\\n",
      "2169         2169  00:27:14:11  00:27:17:18  5300433743159556513.xml      476   \n",
      "15429       15429  01:51:44:15  01:51:49:11  5318609185758554501.xml     1968   \n",
      "18172       18172  02:22:33:07  02:22:37:09  5321170704253895214.xml     2688   \n",
      "8949         8949  01:01:49:22  01:01:53:07  5310822839547632763.xml     1122   \n",
      "17753       17753  01:59:58:22  02:00:03:22  5321170704253895214.xml     2269   \n",
      "14716       14716  01:11:16:24  01:11:19:03  5318609185758554501.xml     1255   \n",
      "11016       11016  00:58:25:02  00:58:30:08  5313398531435045858.xml     1072   \n",
      "18990       18990  00:41:03:04  00:41:06:05  5323778608396028180.xml      725   \n",
      "4203         4203  00:02:11:10  00:02:13:09  5305617339184877785.xml       22   \n",
      "19977       19977  01:37:50:18  01:37:53:12  5323778608396028180.xml     1712   \n",
      "18309       18309  00:03:07:02  00:03:09:05  5323778608396028180.xml       44   \n",
      "17069       17069  01:24:08:21  01:24:10:15  5321170704253895214.xml     1585   \n",
      "19495       19495  01:09:14:01  01:09:26:06  5323778608396028180.xml     1230   \n",
      "1724         1724  00:03:29:08  00:03:32:18  5300433743159556513.xml       31   \n",
      "3468         3468  01:14:21:09  01:14:25:24  5303026185415228226.xml     1224   \n",
      "17062       17062  01:23:45:18  01:23:47:04  5321170704253895214.xml     1578   \n",
      "18390       18390  00:07:45:22  00:07:46:20  5323778608396028180.xml      125   \n",
      "18425       18425  00:10:06:16  00:10:08:02  5323778608396028180.xml      160   \n",
      "7944         7944  00:06:25:06  00:06:31:15  5310822839547632763.xml      117   \n",
      "17543       17543  01:48:49:22  01:48:52:01  5321170704253895214.xml     2059   \n",
      "17618       17618  01:52:19:10  01:52:23:11  5321170704253895214.xml     2134   \n",
      "1579         1579  01:19:33:08  01:19:35:16  5300410550331962313.xml     1580   \n",
      "9389         9389  01:29:15:14  01:29:18:04  5310822839547632763.xml     1562   \n",
      "20022       20022  01:41:49:16  01:41:52:24  5323778608396028180.xml     1757   \n",
      "3179         3179  00:55:30:12  00:55:33:22  5303026185415228226.xml      935   \n",
      "3929         3929  01:40:47:10  01:40:48:01  5303026185415228226.xml     1685   \n",
      "5150         5150  01:01:25:18  01:01:31:13  5305617339184877785.xml      969   \n",
      "3438         3438  01:12:13:18  01:12:14:10  5303026185415228226.xml     1194   \n",
      "18387       18387  00:07:41:13  00:07:43:06  5323778608396028180.xml      122   \n",
      "695           695  00:33:57:19  00:33:59:10  5300410550331962313.xml      696   \n",
      "...           ...          ...          ...                      ...      ...   \n",
      "1534         1534  01:17:24:23  01:17:26:04  5300410550331962313.xml     1535   \n",
      "4186         4186  00:00:34:12  00:00:45:18  5305617339184877785.xml        5   \n",
      "315           315  00:16:09:03  00:16:10:02  5300410550331962313.xml      316   \n",
      "4656         4656  00:29:26:07  00:29:33:00  5305617339184877785.xml      475   \n",
      "2176         2176  00:27:30:12  00:27:40:07  5300433743159556513.xml      483   \n",
      "15191       15191  01:38:53:13  01:38:55:17  5318609185758554501.xml     1730   \n",
      "12295       12295  00:46:06:11  00:46:10:22  5316007724067368375.xml      855   \n",
      "7073         7073  01:06:51:03  01:06:55:10  5308226531817199474.xml     1027   \n",
      "7692         7692  01:49:25:20  01:49:30:14  5308226531817199474.xml     1646   \n",
      "5839         5839  01:44:03:00  01:44:12:18  5305617339184877785.xml     1658   \n",
      "5051         5051  00:55:19:16  00:55:21:15  5305617339184877785.xml      870   \n",
      "6097         6097  00:03:40:09  00:03:44:17  5308226531817199474.xml       51   \n",
      "4755         4755  00:35:21:06  00:35:23:24  5305617339184877785.xml      574   \n",
      "3689         3689  01:28:10:17  01:28:15:22  5303026185415228226.xml     1445   \n",
      "16385       16385  00:50:57:21  00:50:59:17  5321170704253895214.xml      901   \n",
      "14316       14316  00:47:21:11  00:47:27:10  5318609185758554501.xml      855   \n",
      "4932         4932  00:46:05:05  00:46:06:14  5305617339184877785.xml      751   \n",
      "2504         2504  00:16:25:01  00:16:28:04  5303026185415228226.xml      260   \n",
      "7695         7695  01:49:38:01  01:49:39:09  5308226531817199474.xml     1649   \n",
      "12758       12758  01:11:08:17  01:11:10:05  5316007724067368375.xml     1318   \n",
      "20122       20122  01:47:24:00  01:47:25:03  5323778608396028180.xml     1857   \n",
      "16263       16263  00:44:34:20  00:44:38:21  5321170704253895214.xml      779   \n",
      "2991         2991  00:44:08:04  00:44:12:10  5303026185415228226.xml      747   \n",
      "9102         9102  01:12:03:08  01:12:06:13  5310822839547632763.xml     1275   \n",
      "6283         6283  00:14:38:13  00:14:41:05  5308226531817199474.xml      237   \n",
      "8735         8735  00:50:28:22  00:50:29:18  5310822839547632763.xml      908   \n",
      "7419         7419  01:31:15:23  01:31:18:13  5308226531817199474.xml     1373   \n",
      "10333       10333  00:21:26:08  00:21:29:06  5313398531435045858.xml      389   \n",
      "7384         7384  01:29:10:03  01:29:12:06  5308226531817199474.xml     1338   \n",
      "10618       10618  00:37:07:04  00:37:18:15  5313398531435045858.xml      674   \n",
      "\n",
      "                                              transcript Death relative  \\\n",
      "2169                                     My mum's dead.         99.8199   \n",
      "15429  No!  Look I told you, Granddad,  it's going to...        99.8053   \n",
      "18172             Her mother is dead!  Dead and buried!         99.8028   \n",
      "8949             Your dad will be  turning in his grave.        99.7833   \n",
      "17753                        I better get over the Vic.         99.7453   \n",
      "14716                                        Freddie...         99.7007   \n",
      "11016           Yeah...  Well...  I'm sorry Sean's gone.        99.6555   \n",
      "18990  the other horn player?  She passed away in the...        99.6087   \n",
      "4203              Garry's never comin' back,  all right?        99.5513   \n",
      "19977                          For Herbert, not for me!         99.5481   \n",
      "18309                           Stay away from the Vic.         99.5326   \n",
      "17069  No.  Come here,  come and give your old dad a ...        99.5194   \n",
      "19495             I can't bear it, Dad.  Neither can I.         99.5112   \n",
      "1724   But then, when she died...  I decided to come ...        99.4754   \n",
      "3468                                         Oh, Hev...         99.4671   \n",
      "17062                                  Goodbye, Janine.         99.4659   \n",
      "18390  No, no-one'd take you in.  Auntie  Peggy'd kil...        99.4301   \n",
      "18425  Have you had the Old Bill  on the phone yet? W...        99.4119   \n",
      "7944   Stacey.  Did you just say  your mum's not comi...        99.4041   \n",
      "17543                         He told me that she died.         99.4015   \n",
      "17618            You told Ronnie  her daughter was dead!        99.3907   \n",
      "1579      then I'm gonna help  my nan pack up the stall.        99.3377   \n",
      "9389   I'm just checking.  I don't want to hear about...        99.3195   \n",
      "20022  Please, please, please, you  know I don't want...        99.3184   \n",
      "3179     That's the old Hev talking!  Remember, no fear.        99.2887   \n",
      "3929   It was George's!  I need it!  Will you just br...        99.2809   \n",
      "5150   Where've you been?!  Talking to the dead man. ...        99.1813   \n",
      "3438   You saw George Michael?  Where?  Where did you...        99.1175   \n",
      "18387             It's too late, Dad.  Don't you get it?        99.0398   \n",
      "695    Granny Linda...  Shh!  Granny Linda's not very...        99.0333   \n",
      "...                                                  ...            ...   \n",
      "1534                                   It's the police.       0.0682073   \n",
      "4186                      # I'm forever blowing bubbles        0.118359   \n",
      "315                          Alterations?  To the house.         0.8808   \n",
      "4656   Playing hide and seek  with her dolls. Here, M...       0.262089   \n",
      "2176                        # Happy birthday to you.  #       0.0493411   \n",
      "15191                                      What?  Magic.       0.182273   \n",
      "12295                            # From Lullaby Bay.  #        0.176514   \n",
      "7073     I'm gonna start the cooking now.  Don't bother.      0.0923198   \n",
      "7692           # Happy birthday...  # Happy birthday...        0.376854   \n",
      "5839   sells fruit and veg off a barrow  and thinks h...      0.0718946   \n",
      "5051                                      Like origami?        0.193424   \n",
      "6097                             Oh, the exercise bike?       0.0822643   \n",
      "4755          You're just a kid.  This is for grown-ups.      0.0549261   \n",
      "3689                   A bit of saliva on a cotton bud.       0.0697579   \n",
      "16385              I just fancied a drink with friends.       0.0430549   \n",
      "14316              The story of Notting Hill back then.        0.590605   \n",
      "4932                                       The library?        0.574344   \n",
      "2504   Poor Garry.  I bet he's well confused. Garry? ...       0.167443   \n",
      "7695                             # Happy birthday...  #       0.0863745   \n",
      "12758                                 It's my birthday.        0.114973   \n",
      "20122  I won't even ask.  I'm looking for caterpillar...       0.155577   \n",
      "16263                   What sort of things?  Souvenirs.       0.208088   \n",
      "2991                  Cos the theme was Club Tropicana!       0.0645912   \n",
      "9102                       Tonics are the fridge.  Right       0.141211   \n",
      "6283   And we were good together.  You know that. No,...       0.187808   \n",
      "8735             Estimate?  That's my philately archive.       0.156051   \n",
      "7419   Hugh Sylvia.  Posh.  Wet.  Not him.  Samantha ...       0.139233   \n",
      "10333                     What's this?  My pilates ball.       0.018428   \n",
      "7384         Just to say Happy Birthday.  Oh, thank you.      0.0817566   \n",
      "10618  Peggy Mitchell!  As I live and breathe. Where ...      0.0451189   \n",
      "\n",
      "          Divorce Marital separation Imprisonment  ... New family member  \\\n",
      "2169     0.459608            6.13362      6.02427  ...          0.224134   \n",
      "15429     15.1892            19.4342       7.7047  ...           7.27385   \n",
      "18172     1.81735            9.93585      4.86592  ...         0.0462602   \n",
      "8949      18.8726            19.6866      15.9079  ...          0.119913   \n",
      "17753     4.22577            29.9484     0.888258  ...           1.63286   \n",
      "14716     0.92441             19.197      4.07233  ...           32.3863   \n",
      "11016     63.8211             58.302      11.3011  ...           5.45401   \n",
      "18990     14.1221            45.2269     0.165218  ...          0.136562   \n",
      "4203      57.9613            58.4353      46.5257  ...           2.05363   \n",
      "19977     25.4477            51.6128      9.69525  ...           33.5076   \n",
      "18309     2.81472            32.6554      14.4653  ...           14.2083   \n",
      "17069     1.48346            10.0982      1.71346  ...            5.0379   \n",
      "19495     49.9418            46.1422      31.1078  ...             10.94   \n",
      "1724      12.0743            38.5618      3.36931  ...           22.9543   \n",
      "3468      8.33205             22.553      7.95034  ...           31.1351   \n",
      "17062     63.4311            64.0813      6.99413  ...           25.2758   \n",
      "18390     27.2239            39.3368      34.6816  ...           28.5295   \n",
      "18425    0.830158            6.55423      1.25382  ...         0.0104115   \n",
      "7944      36.2655            61.4699      13.6829  ...           36.4896   \n",
      "17543     21.0173            54.7412      0.61196  ...           21.1645   \n",
      "17618     5.95484            25.1815      4.89591  ...           3.78647   \n",
      "1579      3.75384            29.5489     0.911081  ...           32.6456   \n",
      "9389     0.665356            3.50254      10.5461  ...           16.9296   \n",
      "20022     9.73158            41.0135      11.6191  ...           35.9313   \n",
      "3179      7.47179            12.2711      7.70218  ...        0.00300986   \n",
      "3929      2.80218            30.4681      5.63206  ...           1.84586   \n",
      "5150      5.54587            18.1535      9.89706  ...         0.0133309   \n",
      "3438    0.0122059           0.145684    0.0424467  ...           31.0204   \n",
      "18387     38.7391            41.7225      16.6002  ...           8.75066   \n",
      "695       1.53137            4.20366      2.19068  ...          0.197059   \n",
      "...           ...                ...          ...  ...               ...   \n",
      "1534    0.0296849          0.0472096     0.345535  ...         0.0249252   \n",
      "4186    0.0392289           0.546915    0.0196196  ...           1.19188   \n",
      "315      0.142351           0.458938     0.169805  ...          0.838056   \n",
      "4656    0.0210837           0.011736    0.0215317  ...            1.1481   \n",
      "2176   0.00433318          0.0122926    0.0202363  ...          0.158199   \n",
      "15191  0.00509444          0.0549157   0.00728553  ...           1.01446   \n",
      "12295   0.0136122           0.034511     0.016257  ...          0.301417   \n",
      "7073    0.0242983           0.102322    0.0151054  ...          0.972634   \n",
      "7692   0.00270466         0.00716344   0.00574261  ...          0.926957   \n",
      "5839   0.00612764          0.0157911   0.00283714  ...          0.560248   \n",
      "5051   0.00762646          0.0305694   0.00787124  ...          0.649997   \n",
      "6097   0.00579875          0.0207629   0.00462488  ...          0.892024   \n",
      "4755     0.430818           0.455853     0.688577  ...         0.0973107   \n",
      "3689    0.0127233          0.0268476    0.0115145  ...           0.60319   \n",
      "16385   0.0189536          0.0201587    0.0167383  ...         0.0524497   \n",
      "14316   0.0196822           0.112239     0.082189  ...          0.235463   \n",
      "4932    0.0163841          0.0239758    0.0105536  ...         0.0388808   \n",
      "2504    0.0574034           0.148092     0.059607  ...          0.390413   \n",
      "7695   0.00571467           0.022756    0.0208857  ...          0.149062   \n",
      "12758  0.00506348          0.0091884     0.010075  ...          0.065148   \n",
      "20122    0.027397            0.10903    0.0741548  ...          0.381962   \n",
      "16263  0.00576033          0.0138683   0.00630728  ...          0.383199   \n",
      "2991   0.00647124          0.0109562   0.00812208  ...          0.322406   \n",
      "9102    0.0124204          0.0227793    0.0355937  ...          0.165455   \n",
      "6283    0.0279534          0.0945277     0.026222  ...          0.070051   \n",
      "8735    0.0779563          0.0670951    0.0577994  ...         0.0054338   \n",
      "7419    0.0161078          0.0606679     0.014169  ...         0.0414415   \n",
      "10333  0.00620128         0.00521058    0.0105501  ...         0.0308303   \n",
      "7384   0.00324186          0.0100021   0.00532626  ...          0.119667   \n",
      "10618  0.00419882         0.00743917   0.00368654  ...         0.0034475   \n",
      "\n",
      "      Business readjustment Money change Work change     Arguing    Mortgage  \\\n",
      "2169               0.310089     0.368668    0.983882     9.24417    0.470136   \n",
      "15429               16.1709      21.8044     23.0551     30.9171     16.4768   \n",
      "18172               1.16459    0.0423924     1.71088     15.0373    0.450173   \n",
      "8949                4.91555      6.16511     12.6837     14.6446     10.1234   \n",
      "17753                13.523      1.16922     7.44457     3.02713    0.330589   \n",
      "14716               7.54954      8.44094     10.9915     5.59523      31.853   \n",
      "11016               24.9392      26.5578     34.1166     26.1382     9.83673   \n",
      "18990              0.309167    0.0419095     2.61222     4.35367    0.111108   \n",
      "4203                3.91721      1.56989     13.1612     19.1925    0.702606   \n",
      "19977               23.1407      20.9741     30.4518     29.2808     4.26868   \n",
      "18309               4.57361      3.08265     6.86762     9.49288    0.239519   \n",
      "17069               11.0569     0.810583     16.3006     22.2966     6.98161   \n",
      "19495               7.89603      12.7898     21.5868     22.5049      18.876   \n",
      "1724                 12.488      19.4401     29.0306     10.0063     3.33739   \n",
      "3468                12.9089       11.559     12.0958     12.0527      5.8174   \n",
      "17062               13.4072      8.40772     24.7617     7.52594    0.830521   \n",
      "18390               15.0539      17.7579     17.9597     28.8872     13.8377   \n",
      "18425               12.6705      5.89587     14.3218     7.86736     3.24687   \n",
      "7944                2.71788     0.209597     14.5914     27.4826    0.385657   \n",
      "17543              0.897287     0.119979     2.45944     2.87207     0.79112   \n",
      "17618               6.65698     0.150813      11.289     24.5211     2.13581   \n",
      "1579                21.9692      12.9465       33.69      1.2339     1.22079   \n",
      "9389                3.86048     0.769392     8.89903     10.9061     7.28196   \n",
      "20022               3.97949      10.0274     13.1408     30.0604     12.6187   \n",
      "3179                5.86697      9.97216     4.38179      15.824     6.87479   \n",
      "3929                5.22823      10.7985     9.64083     27.7807     4.22386   \n",
      "5150                4.82163      2.65887     8.60718     31.9853     9.26224   \n",
      "3438                1.62242     0.231139     7.02374     27.1048    0.153552   \n",
      "18387               10.8603       16.567     18.7768     27.0144      16.278   \n",
      "695                0.499459     0.724173     5.29688     15.1073    0.201327   \n",
      "...                     ...          ...         ...         ...         ...   \n",
      "1534              0.0246225    0.0165033   0.0591942    0.133413   0.0161873   \n",
      "4186               0.493338     0.128304    0.623593    0.165032    0.118638   \n",
      "315                0.110088    0.0897691    0.254722    0.974222     1.17251   \n",
      "4656              0.0204567    0.0124141     0.01829   0.0072943   0.0617365   \n",
      "2176              0.0946676    0.0564188   0.0294844    0.223395     0.25636   \n",
      "15191             0.0276501    0.0334822   0.0396657    0.209154   0.0098637   \n",
      "12295             0.0168206     0.192035   0.0136347   0.0144851  0.00852823   \n",
      "7073               0.128839    0.0167766    0.252626   0.0842868   0.0184116   \n",
      "7692               0.135197    0.0464819   0.0119543   0.0182382    0.402687   \n",
      "5839               0.152562    0.0367243   0.0830665     0.09618   0.0183138   \n",
      "5051              0.0236379   0.00689373   0.0519813   0.0230156    0.011311   \n",
      "6097              0.0251443   0.00373348   0.0798412   0.0622625  0.00298569   \n",
      "4755               0.781109     0.536646    0.201484    0.421723    0.228833   \n",
      "3689              0.0652957   0.00495986    0.097325    0.179802   0.0106672   \n",
      "16385             0.0242061    0.0309084  0.00686986    0.595576   0.0668922   \n",
      "14316             0.0218519    0.0379409   0.0452284   0.0595151   0.0203548   \n",
      "4932              0.0369815    0.0184696   0.0758233    0.481018  0.00805247   \n",
      "2504               0.186549     0.140171    0.550737    0.309359   0.0305658   \n",
      "7695              0.0388985    0.0617217   0.0197884   0.0745491    0.178793   \n",
      "12758             0.0187987    0.0191035   0.0118859    0.346798  0.00780565   \n",
      "20122              0.108087    0.0124665     0.38899     0.30409   0.0247729   \n",
      "16263             0.0214955    0.0036698  0.00649699   0.0171381  0.00265055   \n",
      "2991              0.0148718    0.0212099   0.0129121   0.0156982  0.00735402   \n",
      "9102              0.0970963    0.0363892   0.0642954   0.0316497   0.0117683   \n",
      "6283              0.0494747    0.0234894    0.046309   0.0993841   0.0130691   \n",
      "8735                0.11094     0.091873   0.0865227   0.0315387   0.0581775   \n",
      "7419              0.0248957    0.0293282     0.03999     0.10916  0.00580807   \n",
      "10333             0.0108196   0.00244614   0.0816238    0.134427  0.00495678   \n",
      "7384              0.0170082    0.0236723   0.0121142   0.0204194   0.0131802   \n",
      "10618              0.018029   0.00757102   0.0238577  0.00576272  0.00390689   \n",
      "\n",
      "      Child leaving home Trouble with in-laws Achievement      score  \n",
      "2169             1.87942             0.583255      1.2338  99.819946  \n",
      "15429            23.6184              19.6614     5.85833  99.805349  \n",
      "18172            14.9555              21.5531     0.12317  99.802786  \n",
      "8949             9.70459              18.5237     6.97051  99.783289  \n",
      "17753             3.7303              6.80974    0.311204  99.745327  \n",
      "14716            6.59892              1.58337     19.1424  99.700743  \n",
      "11016            27.9494              0.63017     8.10617  99.655533  \n",
      "18990          0.0134689             0.583611     1.17064  99.608719  \n",
      "4203              26.339              3.75059    0.786608  99.551296  \n",
      "19977            13.2921              4.56578     18.8535  99.548060  \n",
      "18309            11.1086              18.2983    0.254366  99.532557  \n",
      "17069            10.4199              27.7819     8.88511  99.519420  \n",
      "19495              27.54              14.0536     1.41107  99.511236  \n",
      "1724             19.6815              3.84776     12.0931  99.475402  \n",
      "3468             12.6818               7.5447     12.3799  99.467134  \n",
      "17062            27.1652              7.11326     1.30919  99.465907  \n",
      "18390            22.0334              28.2245     5.53098  99.430138  \n",
      "18425          0.0176177              5.86663     3.91429  99.411935  \n",
      "7944             28.6044              28.1199    0.478607  99.404067  \n",
      "17543           0.615694               12.215   0.0785249  99.401468  \n",
      "17618           0.635407              27.5046     15.1811  99.390727  \n",
      "1579              27.178              27.6722     13.0172  99.337715  \n",
      "9389             11.7497              4.58897     2.84836  99.319500  \n",
      "20022            12.4082              9.57707     2.37542  99.318427  \n",
      "3179            0.226215              4.06352     2.27534  99.288750  \n",
      "3929             2.32159              6.89197     14.0888  99.280882  \n",
      "5150            0.111859              1.58818     1.04345  99.181300  \n",
      "3438            0.469176            0.0413175     9.65921  99.117547  \n",
      "18387             22.954              9.63091     1.36778  99.039775  \n",
      "695             0.081853              27.9778    0.197278  99.033272  \n",
      "...                  ...                  ...         ...        ...  \n",
      "1534            0.018349            0.0134304   0.0728839   1.192770  \n",
      "4186           0.0516743            0.0632466    0.860154   1.191879  \n",
      "315             0.533984             0.122796   0.0497411   1.172509  \n",
      "4656           0.0394944           0.00569328    0.301297   1.148099  \n",
      "2176           0.0338281           0.00592261     1.10654   1.106537  \n",
      "15191           0.140497            0.0138869    0.478772   1.014455  \n",
      "12295           0.977092           0.00773617   0.0713183   0.977092  \n",
      "7073            0.268276            0.0980568    0.860671   0.972634  \n",
      "7692             0.12992           0.00258143    0.221188   0.926957  \n",
      "5839           0.0274606            0.0596527    0.904279   0.904279  \n",
      "5051           0.0618183           0.00671439    0.892949   0.892949  \n",
      "6097           0.0405282           0.00792897    0.214135   0.892024  \n",
      "4755            0.509141             0.322128    0.196223   0.781109  \n",
      "3689           0.0144549            0.0249559   0.0479163   0.603190  \n",
      "16385          0.0723486           0.00310912    0.194231   0.595576  \n",
      "14316            0.47976              0.03039    0.121927   0.590605  \n",
      "4932            0.247691            0.0065992    0.105917   0.574344  \n",
      "2504            0.190726             0.110714   0.0781917   0.550737  \n",
      "7695            0.136376           0.00613554    0.517452   0.517452  \n",
      "12758            0.08632            0.0149871    0.489464   0.489464  \n",
      "20122           0.149167              0.07499    0.214614   0.388990  \n",
      "16263          0.0555066           0.00720987   0.0184667   0.383199  \n",
      "2991            0.223429           0.00408999   0.0340684   0.322406  \n",
      "9102           0.0131078            0.0149993   0.0536464   0.193286  \n",
      "6283           0.0567324            0.0518718   0.0473553   0.187808  \n",
      "8735           0.0438043            0.0641397   0.0101879   0.156051  \n",
      "7419           0.0335367            0.0369189     0.10344   0.139233  \n",
      "10333         0.00791647           0.00526761   0.0727689   0.134427  \n",
      "7384            0.123864           0.00309828   0.0205214   0.123864  \n",
      "10618         0.00387748            0.0149867   0.0182535   0.045119  \n",
      "\n",
      "[10526 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "#candidate_labels=['drama']\n",
    "first_results=get_results_several_binary(df_with_text,classifier,candidate_labels)\n",
    "#first_results=get_results(df_with_text,classifier,candidate_labels)\n",
    "cols_to_keep=['Unnamed: 0', 'begin', 'end', 'filename', 'shot_id', 'transcript']\n",
    "first_results['score'] = first_results.drop(cols_to_keep, axis=1).max(axis=1)\n",
    "first_results=first_results.sort_values(by=['score'],ascending=False)\n",
    "\n",
    "#first_results.to_csv('death_relative_Max_Jack_Tanya.csv')\n",
    "first_results.to_csv('life_scale_ranking_Peggy_Archie.csv')\n",
    "\n",
    "print(first_results)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_scores=[]\n",
    "#eps=['s05e10']\n",
    "for ep in eps:\n",
    "    print(ep)\n",
    "    df=pd.read_csv(ep+'_'+method_used+'_similarity.csv')\n",
    "    sequence=df.scene_text\n",
    "    #candidate_labels = ['Crime scene', 'Victim', 'Death cause', 'Perpetrator', 'Evidence', 'Motive']\n",
    "    #candidate_labels=['crime scene', 'victim', 'death cause', 'perpetrator', 'evidence', 'motive']\n",
    "    candidate_labels=['investigation']\n",
    "    #action, drama, horror and romance.\n",
    "    #candidate_labels=['action', 'crime','romance','comedy']\n",
    "    #candidate_labels=['action', 'crime','romance','comedy']\n",
    "    #candidate_labels=joy,sadness, anger,fear, trust,distrust, surprise,anticipation\n",
    "    #candidate_labels=['surprise','anticipation']\n",
    "    #candidate_labels=['thought-provoking']\n",
    "    \n",
    "    #candidate_labels=['Action','Animation','Biography','Comedy','Crime','Drama','Family','Fantasy','Horror','Mystery','Romance','SciFi','Thriller']\n",
    "    #candidate_labels=['Action','Biography','Comedy','Crime','Drama','Family','Fantasy','Horror','Romance','SciFi','Thriller']\n",
    "    #candidate_labels=['anger', 'sadness','joy',feat]\n",
    "    first_results=get_results(df,classifier,candidate_labels)\n",
    "    first_results[['prediction_score','score_hf']]=MinMaxScaler().fit_transform(first_results[['prediction_score','score_hf']])\n",
    "                  \n",
    "    #print(first_results)\n",
    "    #first_results['sum_score']=0.5*first_results['score_hf']+0.5*first_results['prediction_score']\n",
    "    #print(first_results['sum_score'])\n",
    "    #ranked=first_results[first_results.topic=='murder'].sort_values(by=['score'],ascending=False)\n",
    "    first_results['prediction_class']=0\n",
    "    #ranked['prediction_class']=0\n",
    "    #ranked['prediction_class'].iloc[0:len(first_results[first_results.in_summary==1])]=1\n",
    "    #predicted_pos= ranked['scene_id'][ranked['prediction_class']==1]\n",
    "    #predicted_pos=list(predicted_pos)\n",
    "    #print(predicted_pos)\n",
    "    first_results=first_results.sort_values(by=['score_hf'],ascending=False)\n",
    "    #first_results=first_results.sort_values(by=['score_hf'],ascending=False)\n",
    "    first_results['prediction_class'].iloc[0:len(first_results[first_results.in_summary==1])]=1\n",
    "    #first_results.to_csv('combined_scores_kill'+ep+'.csv',index=None)\n",
    "    #first_results.loc[first_results['scene_id'].isin(predicted_pos), 'prediction_huggingface'] = 1 \n",
    "    #print(first_results)\n",
    "    f1_scores.append(f1_score(first_results.in_summary, first_results.prediction_class))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-33e6f88f7c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aspects'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in_summary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction_class'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sum_score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first_results' is not defined"
     ]
    }
   ],
   "source": [
    "#print(first_results[first_results.in_summary==1])\n",
    "\n",
    "\n",
    "print(first_results[['aspects','in_summary','topic','score','prediction_class','sum_score','prediction','prediction_score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2940651856941449\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(f1_scores))\n",
    "\n",
    "print(len(f1_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1_scores_bis=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ep in eps:\n",
    "    df=pd.read_csv('combined_scores_murders'+ep+'.csv')\n",
    "    #df['max']=df[[\"prediction_score\", \"score_hf\"]].max(axis=1)\n",
    "    df['max']=0.5*df['score_hf']+0.5*df['prediction_score']\n",
    "    #df['max'][df.topic=='joy']=0\n",
    "    df=df.sort_values(by=['prediction_score'],ascending=False)\n",
    "    df['prediction_class']=0\n",
    "    df['prediction_class'].iloc[0:len(df[df.in_summary==1])]=1\n",
    "    #print(df[['in_summary','topic','score_hf','prediction_class','sum_score','prediction','prediction_score','max']])\n",
    "    \n",
    "            \n",
    "            \n",
    "    #first_results.to_csv('combined_scores'+ep+'.csv')\n",
    "    #first_results.loc[first_results['scene_id'].isin(predicted_pos), 'prediction_huggingface'] = 1 \n",
    "    #print(first_results)\n",
    "    df=df.sort_values(by=['score_hf'],ascending=False)\n",
    "    df['prediction_class'].iloc[0:len(df[df.in_summary==1])]=1\n",
    "    #first_results.loc[first_results['scene_id'].isin(predicted_pos), 'prediction_huggingface'] = 1 \n",
    "    #print(first_results0\n",
    "    f1_scores_bis.append(f1_score(df.in_summary, df.prediction_class))\n",
    "    \n",
    "\n",
    "\n",
    "print(np.mean(f1_scores_bis))\n",
    "\n",
    "#print(len(f1_scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-c61821b32eb2>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-c61821b32eb2>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    first_results[first_results.topic=='Crime'].sort_values(by=['score'],ascending=False)[]\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f1_scores_emotions=[]\n",
    "for ep in eps:\n",
    "    df=pd.read_csv(ep+'_'+method_used+'_similarity.csv')\n",
    "    sequence=df.scene_text\n",
    "    #candidate_labels = ['Crime scene', 'Victim', 'Death cause', 'Perpetrator', 'Evidence', 'Motive']\n",
    "    candidate_labels=['anger', 'anticipation', 'joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust']\n",
    "    first_results=get_results(df,classifier,candidate_labels)   \n",
    "    first_results['prediction_class']=0\n",
    "    first_results[first_results.topic=='Crime'].sort_values(by=['score'],ascending=False)[]\n",
    "\n",
    "    ranked['prediction_class'].iloc[0:len(first_results[first_results.in_summary==1])]=1\n",
    "    ranked.to_csv('murder_hum_class'+ep+'.csv')\n",
    "    print(ranked)\n",
    "    f1_scores_emotions.append(f1_score(ranked.in_summary, ranked.prediction_class))\n",
    "   \n",
    "classifier(sequence, candidate_labels, multi_class=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "s04e09\n",
      "[0.19999999999999998, 0.2857142857142857, 0.28571428571428575, 0.3157894736842105, 0.3333333333333333, 0.36363636363636365, 0.4, 0.4, 0.4285714285714285, 0.43478260869565216, 0.45454545454545453, 0.47058823529411764, 0.5, 0.5, 0.5, 0.5, 0.5000000000000001, 0.5714285714285714, 0.6, 0.6, 0.6, 0.6153846153846153, 0.6153846153846153, 0.6153846153846153, 0.631578947368421, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.7000000000000001, 0.7142857142857143, 0.7142857142857143, 0.7272727272727273, 0.7272727272727273, 0.7272727272727273, 0.7499999999999999, 0.7499999999999999, 0.8571428571428571, 1.0]\n",
      "   Unnamed: 0  Unnamed: 0.1  scene_id  \\\n",
      "0           8             8         8   \n",
      "1          30            30        30   \n",
      "\n",
      "                                          scene_text  in_summary      aspects  \\\n",
      "0  [' ( Scene opens on a close up of two prints ,...           1  Death cause   \n",
      "1  [' ( CATHERINE puts the bagged vice grips on t...           1       Motive   \n",
      "\n",
      "   prediction  aspect?   topic     score  prediction_class  \n",
      "0           0        1  Murder  0.837703                 1  \n",
      "1           1        1  Murder  0.551375                 1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_f1 = f1_scores.copy()\n",
    "sorted_f1.sort()\n",
    "print(f1_scores[10])\n",
    "print(eps[10])\n",
    "ep=eps[10]\n",
    "print(sorted_f1)\n",
    "df=pd.read_csv('murder_hum_class'+ep+'.csv')\n",
    "\n",
    "#print(df.scene_text[df.in_summary==0].iloc[4])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s05e10\n",
      "Index(['Unnamed: 0', 'scene_id', 'scene_text', 'in_summary', 'aspects',\n",
      "       'prediction_score', 'prediction', 'aspect?'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_all=pd.DataFrame(columns=['Unnamed: 0', 'scene_id', 'scene_text', 'in_summary', 'aspects',\n",
    "       'prediction_score', 'prediction', 'aspect?'])\n",
    "f1_scores=[]\n",
    "eps=['s05e10']\n",
    "for ep in eps:\n",
    "    print(ep)\n",
    "    df=pd.read_csv(ep+'_'+method_used+'_similarity.csv')\n",
    "    print(df.columns)\n",
    "    df_all=df_all.append(df)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps=[]\n",
    "\n",
    "for ep in listdir(path_screenplays_scenes):\n",
    "    annotated_scenes=pd.read_csv(path_screenplays_scenes+'/'+ep)\n",
    "    eps.append(ep.split('.csv')[0])\n",
    "\n",
    "print(eps)\n",
    "    \n",
    "tags_entire_episode=[]\n",
    "\n",
    "\n",
    "eps=['s03e19', 's04e14', 's04e06', 's01e07', 's04e21', 's05e17', 's03e08', 's04e10', 's04e05', 's05e05', 's05e06', 's01e20', 's04e15', 's02e06', 's01e08', 's04e12', 's05e08', 's03e05', 's02e10', 's05e13', 's03e11', 's01e13']\n",
    "\n",
    "for ep in eps:\n",
    "    print(ep)\n",
    "    df=pd.read_csv(ep+'_'+method_used+'_similarity.csv')\n",
    "    #print(df)\n",
    "    #candidate_labels = ['Crime scene', 'Victim', 'Death cause', 'Perpetrator', 'Evidence', 'Motive']\n",
    "    #candidate_labels=['crime scene', 'victim', 'death cause', 'perpetrator', 'evidence', 'motive']\n",
    "    #candidate_labels=['kill','love']\n",
    "    #action, drama, horror and romance.\n",
    "    #candidate_labels=['action', 'crime','romance','comedy','murder']\n",
    "    #candidate_labels=['action', 'crime','romance','comedy']\n",
    "    #candidate_labels=joy,sadness, anger,fear, trust,distrust, surprise,anticipation\n",
    "    candidate_labels=['murder','violence', 'flashback', 'romantic', 'cult', 'revenge', 'psychedelic', 'comedy', 'suspenseful', 'good_versus_evil', 'humor', 'satire', 'entertaining', 'neo noir', 'action', 'sadist', 'insanity', 'tragedy', 'fantasy', 'paranormal', 'boring', 'mystery', 'horror', 'melodrama', 'cruelty', 'gothic', 'dramatic', 'dark', 'atmospheric', 'sci-fi', 'psychological', 'historical', 'absurd', 'prank', 'sentimental', 'philosophical', 'avant garde', 'bleak', 'alternate reality', 'depressing', 'plot twist', 'realism', 'cute', 'stupid', 'intrigue', 'pornographic', 'home movie', 'haunting', 'historical fiction', 'allegory', 'adult comedy', 'inspiring', 'anti war', 'comic', 'brainwashing', 'alternate history', 'queer', 'clever', 'claustrophobic', 'whimsical', 'feel-good', 'blaxploitation', 'western', 'grindhouse film', 'magical realism', 'suicidal', 'autobiographical', 'christian film', 'non fiction']\n",
    "    df['topic'] = None\n",
    "    for i in tqdm(range(0, df.shape[0])):\n",
    "        df.loc[i, 'topic']='bigoudi'\n",
    "        text = df.loc[i, 'scene_text']\n",
    "        result= classifier(text, 'murder')\n",
    "        if text is not None:\n",
    "            for j in candidate_labels:\n",
    "                result_provisoire = classifier(text, j)\n",
    "                #print(result_provisoire['scores'][0].type)\n",
    "                #print(result_provisoire['scores'][0])\n",
    "                #print(result['scores'][0])\n",
    "                if result_provisoire['scores'][0]>result['scores'][0]:\n",
    "                    result['scores']=result_provisoire['scores']\n",
    "                    #print(result['scores'])\n",
    "                    df.loc[i, 'topic'] = result_provisoire['labels'][0]\n",
    "                    #print(df.loc[i, 'topic'])                   \n",
    "            #print(result['labels'][0])\n",
    "    #print(df.loc[i, 'topic'])\n",
    "    print(df['topic'].value_counts().argmax())\n",
    "    tags_entire_episode.append(df['topic'].value_counts().argmax())\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    sequence=df.scene_text\n",
    "    #candidate_labels = ['Crime scene', 'Victim', 'Death cause', 'Perpetrator', 'Evidence', 'Motive']\n",
    "    #candidate_labels=['crime scene', 'victim', 'death cause', 'perpetrator', 'evidence', 'motive']\n",
    "    #candidate_labels=['kill']\n",
    "    #action, drama, horror and romance.\n",
    "    #candidate_labels=['action', 'crime','romance','comedy']\n",
    "    #candidate_labels=['action', 'crime','romance','comedy']\n",
    "    #candidate_labels=joy,sadness, anger,fear, trust,distrust, surprise,anticipation\n",
    "    candidate_labels=['surprise','anticipation']\n",
    "    \n",
    "    #candidate_labels=['Action','Animation','Biography','Comedy','Crime','Drama','Family','Fantasy','Horror','Mystery','Romance','SciFi','Thriller']\n",
    "    #candidate_labels=['Action','Biography','Comedy','Crime','Drama','Family','Fantasy','Horror','Romance','SciFi','Thriller']\n",
    "    #candidate_labels=['anger', 'sadness','joy',feat]\n",
    "    first_results=get_results(df,classifier,candidate_labels)\n",
    "    first_results[['prediction_score','score_hf']]=MinMaxScaler().fit_transform(first_results[['prediction_score','score_hf']])\n",
    "                  \n",
    "    #print(first_results)\n",
    "    first_results['sum_score']=0.5*first_results['score_hf']+0.5*first_results['prediction_score']\n",
    "    #print(first_results['sum_score'])\n",
    "    #ranked=first_results[first_results.topic=='murder'].sort_values(by=['score'],ascending=False)\n",
    "    first_results['prediction_class']=0\n",
    "    #ranked['prediction_class']=0\n",
    "    #ranked['prediction_class'].iloc[0:len(first_results[first_results.in_summary==1])]=1\n",
    "    #predicted_pos= ranked['scene_id'][ranked['prediction_class']==1]\n",
    "    #predicted_pos=list(predicted_pos)\n",
    "    #print(predicted_pos)\n",
    "    #first_results=first_results.sort_values(by=['sum_score'],ascending=False)\n",
    "    first_results=first_results.sort_values(by=['score_hf'],ascending=False)\n",
    "    first_results['prediction_class'].iloc[0:len(first_results[first_results.in_summary==1])]=1\n",
    "    #first_results.to_csv('combined_scores_kill'+ep+'.csv',index=None)\n",
    "    #first_results.loc[first_results['scene_id'].isin(predicted_pos), 'prediction_huggingface'] = 1 \n",
    "    #print(first_results)\n",
    "    f1_scores.append(f1_score(first_results.in_summary, first_results.prediction_class))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGXwxxyn9nOC"
   },
   "source": [
    "To do multi-class classification, simply pass `multi_class=True`. In this case, the scores will be independent, but each will fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "ZvZeVb2h5RX0",
    "outputId": "9ba085bf-4c52-4011-9c51-3a0adeddd3a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['politics', 'elections', 'public health', 'economics'],\n",
       " 'scores': [0.972069501876831,\n",
       "  0.967610776424408,\n",
       "  0.03248710557818413,\n",
       "  0.0061644683592021465],\n",
       " 'sequence': 'Who are you voting for in 2020?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"politics\", \"public health\", \"economics\", \"elections\"]\n",
    "\n",
    "classifier(sequence, candidate_labels, multi_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLLeDT1r9-yQ"
   },
   "source": [
    "Here's an example of sentiment classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "f7AF53Wl5f8W",
    "outputId": "50a52076-7d2b-4ce0-b95f-c9cf9a13b361"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['negative', 'positive'],\n",
       " 'scores': [0.9916268587112427, 0.00837317667901516],\n",
       " 'sequence': 'I hated this movie. The acting sucked.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"I hated this movie. The acting sucked.\"\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "classifier(sequence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSoBpCpV6k4s"
   },
   "source": [
    "So how does this method work?\n",
    "\n",
    "The underlying model is trained on the task of Natural Language Inference (NLI), which takes in two sequences and determines whether they contradict each other, entail each other, or neither.\n",
    "\n",
    "This can be adapted to the task of zero-shot classification by treating the sequence which we want to classify as one NLI sequence (called the premise) and turning a candidate label into the other (the hypothesis). If the model predicts that the constructed premise _entails_ the hypothesis, then we can take that as a prediction that the label applies to the text. Check out [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more detailed explanation.\n",
    "\n",
    "By default, the pipeline turns labels into hypotheses with the template `This example is {class_name}.`. This works well in many settings, but you can also customize this for your specific setting. Let's add another review to our above sentiment classification example that's a bit more challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "5yLx3pRr5xQA",
    "outputId": "6420fb46-9aeb-4055-8ab6-fdc5eb822a60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': ['negative', 'positive'],\n",
       "  'scores': [0.9916267991065979, 0.008373182266950607],\n",
       "  'sequence': 'I hated this movie. The acting sucked.'},\n",
       " {'labels': ['negative', 'positive'],\n",
       "  'scores': [0.8148515820503235, 0.1851484179496765],\n",
       "  'sequence': \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I hated this movie. The acting sucked.\",\n",
    "    \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"\n",
    "]\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "classifier(sequences, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfrpyGWM782R"
   },
   "source": [
    "The second example is a bit harder. Let's see if we can improve the results by using a hypothesis template which is more specific to the setting of review sentiment analysis. Instead of the default, `This example is {}.`, we'll use, `The sentiment of this review is {}.` (where `{}` is replaced with the candidate class name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "kqx5hp7X8XNA",
    "outputId": "69c6e083-f3dc-41db-fca5-d96a3541f1fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': ['negative', 'positive'],\n",
       "  'scores': [0.9890093207359314, 0.010990672744810581],\n",
       "  'sequence': 'I hated this movie. The acting sucked.'},\n",
       " {'labels': ['positive', 'negative'],\n",
       "  'scores': [0.9581228494644165, 0.0418771356344223],\n",
       "  'sequence': \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I hated this movie. The acting sucked.\",\n",
    "    \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"\n",
    "]\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "hypothesis_template = \"The sentiment of this review is {}.\"\n",
    "\n",
    "classifier(sequences, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iArbRAe781-_"
   },
   "source": [
    "By providing a more precise hypothesis template, we are able to see a more accurate classification of the second review.\n",
    "\n",
    "> Note that sentiment classification is used here just as an illustrative example. The [Hugging Face Model Hub](https://huggingface.co/models?filter=text-classification) has a number of models trained specifically on sentiment tasks which can be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxUTOnllSH4w"
   },
   "source": [
    "#### Update: Zero-shot classification in 100 languages\n",
    "\n",
    "Interested in using the pipeline for languages other than English? We've trained a cross-lingual model on top of XLM RoBERTa which you can use by passing `model='joeddav/xlm-roberta-large-xnli'` when creating the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siZhFPekSN7t"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrcljZ75UxKN"
   },
   "source": [
    "You can use it with any combination of languages. For example, let's classify a Russian sentence with English candidate labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "gBJyFwC2TwGv",
    "outputId": "5e683b8a-2e9f-46c2-95f9-04559bed04ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['politics', 'Europe', 'public health'],\n",
       " 'scores': [0.9048484563827515, 0.05722189322113991, 0.03792969882488251],\n",
       " 'sequence': 'За кого вы голосуете в 2020 году?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"За кого вы голосуете в 2020 году?\" # translation: \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"Europe\", \"public health\", \"politics\"]\n",
    "\n",
    "classifier(sequence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9hGVMsrVI8S"
   },
   "source": [
    "Now let's do the same but with the labels in French:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "0xKYBOLYVeNJ",
    "outputId": "a8066bb5-9e95-4f80-d77b-9753cc4c4be3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['politique', 'Europe', 'santé publique'],\n",
       " 'scores': [0.9726154804229736, 0.017128489911556244, 0.010256024077534676],\n",
       " 'sequence': 'За кого вы голосуете в 2020 году?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"За кого вы голосуете в 2020 году?\" # translation: \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"Europe\", \"santé publique\", \"politique\"]\n",
    "\n",
    "classifier(sequence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHURJUPCVgGP"
   },
   "source": [
    "As we discussed in the last section, the default hypothesis template is the English, `This text is {}.`. If you are working strictly within one language, it may be worthwhile to translate this to the language you are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "ZCtTclt7VpMv",
    "outputId": "87c768b3-2193-4164-d993-0ee9c9eec3ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['política', 'Europa', 'salud pública'],\n",
       " 'scores': [0.9109585881233215, 0.05954807624220848, 0.029493311420083046],\n",
       " 'sequence': '¿A quién vas a votar en 2020?'}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"¿A quién vas a votar en 2020?\"\n",
    "candidate_labels = [\"Europa\", \"salud pública\", \"política\"]\n",
    "hypothesis_template = \"Este ejemplo es {}.\"\n",
    "\n",
    "classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQyVNE2fTDVs"
   },
   "source": [
    "The model is fine-tuned on XNLI which includes 15 languages: Arabic, Bulgarian, Chinese, English, French, German, Greek, Hindi, Russian, Spanish, Swahili, Thai, Turkish, Urdu, and Vietnamese. The base model is trained on 85 more, so the model will work to some degree for any of those in the XLM RoBERTa training corpus (see the full list in appendix A of the [XLM Roberata paper](https://arxiv.org/abs/1911.02116)).\n",
    "\n",
    "See the [model page](https://huggingface.co/joeddav/xlm-roberta-large-xnli) for more."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "🤗 Zero Shot Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
